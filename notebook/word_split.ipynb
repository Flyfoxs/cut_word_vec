{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Split Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解决如下几个问题\n",
    "- 裁剪词库: \n",
    "\n",
    "开源词库特别大,加载慢,如何根据当前的任务裁剪. 因为字典表非常大,比如腾讯开源的词库有8,824,330个词, 解压后有16G. 针对当前的训练数据往往只是很小一部分,在训练时完全加载这么大的数据集是完全没有必要的,可以提前准备一个mini的字典.\n",
    "\n",
    "\n",
    "- 减少oov的大小:\n",
    "\n",
    "因为中文有分词的问题, 这样分词结果可能和字典表不匹配,导致oov,但是可以通过变通的办法取近似值. 比如\"2019年年底\",可以通过\"2019\" 和 \"年底\"来取平均值 \n",
    "\n",
    "\n",
    "\n",
    "# 腾讯词向量下载地址\n",
    "\n",
    "- https://ai.tencent.com/ailab/nlp/data/Tencent_AILab_ChineseEmbedding.tar.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-prd-05\r\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the working folder\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#print(globals())\n",
    "file_folder = globals()['_dh'][0]\n",
    "wk_dir = os.path.dirname(file_folder)\n",
    "os.chdir(wk_dir)\n",
    "\n",
    "\n",
    "# from code_felix.utils_.util_pandas import *\n",
    "\n",
    "# from code_felix.feature.category import *\n",
    "import matplotlib.pyplot as plt\n",
    "# from code_felix.feature.read_file import _get_transaction, _summary_card_trans_col\n",
    "\n",
    "#trans_new =  get_trans(trans_new_file)\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "train_file='./input/data_train.csv'\n",
    "word2vec_model = './Tencent_AILab_ChineseEmbedding.txt'\n",
    "jieba_dict = './input/jieba.txt'\n",
    "\n",
    "vector_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin load dit\n",
      "end load dit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode:\n",
      "欧阳/建国/是/创新/办/主任/也/是/欢聚/时代/公司/云/计算/方面/的/专家\n",
      "Full Mode:\n",
      "欧阳/欧阳建/建国/国是/创新/办/主任/也/是/欢聚/时代/代公/公司/云/计算/方面/的/专家\n",
      "Research Mode:\n",
      "欧阳/建国/是/创新/办/主任/也/是/欢聚/时代/公司/云/计算/方面/的/专家\n",
      "CPU times: user 591 ms, sys: 53.2 ms, total: 645 ms\n",
      "Wall time: 650 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import jieba\n",
    "\n",
    "print('begin load dit')\n",
    "#jieba.load_userdict(jieba_dict)\n",
    "print('end load dit')\n",
    "text = \"欧阳建国是创新办主任也是欢聚时代公司云计算方面的专家\"\n",
    "#text = \"真的挺好用, 吃不上\"\n",
    "\n",
    "# jieba.cut() 方法接受两个输入参数: \n",
    "# 需要分词的字符串\n",
    "# cut_all 参数用来控制是否采用全模式\n",
    "\n",
    "# 精确模式，默认模式就是精确模式\n",
    "seg_list = jieba.cut(text, cut_all = False)\n",
    "print('Default Mode:\\n' + '/' .join(seg_list))\n",
    "\n",
    "# 全模式\n",
    "seg_list = jieba.cut(text, cut_all = True)\n",
    "print( \"Full Mode:\\n\" + '/' .join(seg_list))\n",
    "\n",
    "# jieba.cut_for_search() 方法接受一个参数：\n",
    "# 需要分词的字符串\n",
    "# 该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细\n",
    "\n",
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(text)\n",
    "print('Research Mode:\\n' + '/'.join(seg_list))\n",
    " \n",
    "list(jieba.dt.FREQ.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/eray1yildiz/using-lstms-with-attention-for-emotion-recognition/notebook\n",
    "# https://ai.tencent.com/ailab/nlp/embedding.html\n",
    "#blog : https://blog.csdn.net/sinat_26917383/article/details/83999966\n",
    "\n",
    "#如何利用词库,增强jieba: https://blog.csdn.net/chinatelecom08/article/details/84588071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Jieba 对训练数据做分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "['买', '这套', '系统', '本来', '是', '用来', '做', '我们', '公司', '的']\n",
      "CPU times: user 248 ms, sys: 30.9 ms, total: 279 ms\n",
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import jieba\n",
    "train= pd.read_csv(train_file, encoding='gb18030', delimiter='\\t', header=None)\n",
    "train = train[:20]\n",
    "train.head()\n",
    "train['jieba'] = train.iloc[:,2].apply(lambda text: ' '.join(jieba.cut(str(text), cut_all = False)))\n",
    "train['jieba_len'] = train['jieba'].apply(lambda text: len(text.split(' ')))\n",
    "train.head()\n",
    "\n",
    "import collections\n",
    "count = collections.Counter()\n",
    "\n",
    "for text in train['jieba'].values:\n",
    "    for word in text.split(' '):\n",
    "        count[word] +=1\n",
    "\n",
    "print(len(count))\n",
    "print(list(count)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 02:22:55,591 util_log.py[92] INFO Start the program at:ai-prd-05, 173.36.99.83, with:Load module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "CPU times: user 152 ms, sys: 28.5 ms, total: 181 ms\n",
      "Wall time: 339 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from file_cache.utils.util_log import *\n",
    "\n",
    "@timed()\n",
    "def gen_mini_embedding(wv_from_text, word_list):\n",
    "    from multiprocessing.dummy import Pool\n",
    "\n",
    "    from functools import partial\n",
    "\n",
    "    partition_num = 8\n",
    "    import math\n",
    "    partition_length = math.ceil(len(word_list)/partition_num)\n",
    "\n",
    "    partition_list = [ word_list[i:i+partition_length]  for i in range(0, len(word_list), partition_length )]\n",
    "    logger.debug(f'The word list split to {len(partition_list)} partitions:{[ len(partition) for partition in partition_list]}')\n",
    "    thread_pool = Pool(processes=partition_num)\n",
    "    process = partial(gen_mini_partition,wv_from_text=wv_from_text )\n",
    "\n",
    "    wv_list = thread_pool.map(process, partition_list)\n",
    "    thread_pool.close(); thread_pool.join()\n",
    "\n",
    "    del wv_from_text\n",
    "\n",
    "    return pd.concat(wv_list)\n",
    "\n",
    "\n",
    "@timed()\n",
    "#直接使用KeyedVectors.load_word2vec_format 加载非常慢\n",
    "def load_embedding(path):\n",
    "    embedding_index = {}\n",
    "    f = open(path,encoding='utf8')\n",
    "    for index,line in enumerate(f):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:],dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "    f.close()\n",
    "    return embedding_index\n",
    "\n",
    "@timed()\n",
    "def filter_duplicate_words(file_list, jieba_dict=None):\n",
    "    import jieba\n",
    "    if jieba_dict:\n",
    "        jieba.load_userdict(jieba_dict)\n",
    "\n",
    "        logger.debug(f'load jieba dict:{len(list(jieba.dt.FREQ.keys()))} from file:{jieba_dict}')\n",
    "    word_count = 0\n",
    "    word_set = set()\n",
    "    for cut_all in [True, False]:\n",
    "        for file in file_list:\n",
    "            input_text = pd.read_csv(file, encoding='gb18030', delimiter='\\t', header=None)\n",
    "            input_text['jieba'] = input_text.iloc[:, 2].apply(lambda text: ' '.join(jieba.cut(str(text), cut_all)))\n",
    "            for index, text in enumerate(input_text['jieba'].values):\n",
    "                for word in text.strip().split(' '):\n",
    "                    word_set.add(word)\n",
    "                    word_count += 1\n",
    "            logger.debug(f'There are {len(word_set)} words after file:{file}')\n",
    "    logger.debug(f'There are {len(word_set)} word were parser from word_count:{word_count} file_list:{file_list}')\n",
    "    return sorted(list(word_set))\n",
    "\n",
    "\n",
    "#@timed()\n",
    "def gen_mini_partition( word_set,  wv_from_text, local=False):\n",
    "    if local:\n",
    "        word_set = word_set[:3000]\n",
    "        logger.debug(\"Run app with local model\")\n",
    "\n",
    "    mini = pd.DataFrame( np.zeros((len(word_set), vector_size)),  index=word_set, )\n",
    "    #for i in tqdm(range(len(word_set))):\n",
    "    for i in range(len(word_set)):\n",
    "        word = word_set[i]\n",
    "        vector = wordVec(word, wv_from_text, 1, 3)\n",
    "        if vector is not None:\n",
    "            mini.loc[word] = vector\n",
    "        else:\n",
    "            logger.debug(f'Can not find vec for:{len(word)},{word}')\n",
    "            mini.loc[word] = np.zeros(vector_size)\n",
    "\n",
    "    return mini\n",
    " \n",
    "\n",
    "def compute_ngrams(word, min_n, max_n):\n",
    "    # BOW, EOW = ('<', '>')  # Used by FastText to attach to all words as prefix and suffix\n",
    "    extended_word = word\n",
    "    ngrams = []\n",
    "    for ngram_length in range(min_n, min(len(extended_word), max_n) + 1):\n",
    "        for i in range(0, len(extended_word) - ngram_length + 1):\n",
    "            ngrams.append(extended_word[i:i + ngram_length])\n",
    "    res =  list(set(ngrams))\n",
    "    return res\n",
    " \n",
    "    \n",
    "#中文特有,因为如果分词总会有一些不再字典里面,并且这个问题会比英语更严重\n",
    "#但是可以通过ngrams来求平均值来得到一个平均的vector\n",
    "def wordVec(word,wv_from_text:dict,min_n = 1, max_n = 3):\n",
    "    '''\n",
    "    ngrams_single/ngrams_more,主要是为了当出现oov的情况下,最好先不考虑单字词向量\n",
    "    '''\n",
    "\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in wv_from_text:\n",
    "        return wv_from_text[word]\n",
    "    else:  \n",
    "        word_size = vector_size\n",
    "        # 计算word的ngrams词组\n",
    "        ngrams = compute_ngrams(word,min_n = min_n, max_n = max_n)\n",
    "        # 不在词典的情况下\n",
    "        word_vec = np.zeros(word_size, dtype=np.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in wv_from_text:\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                if ngram in wv_from_text:\n",
    "                    word_vec += wv_from_text[ngram]\n",
    "                    ngrams_found += 1\n",
    "                elif ngram.lower() in wv_from_text:\n",
    "                    word_vec += wv_from_text[ngram.lower()]\n",
    "                    ngrams_found += 1\n",
    "                else:\n",
    "                    logger.warning(f'Can not find {ngram} in wv')\n",
    "        if ngrams_found > 0:\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            logger.error('all ngrams for word \"%s\" absent from model' % word)\n",
    "            return None\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 02:22:55,928 util_log.py[47] INFO load_embedding begin with(1 paras) :['./Tencent_AILab_ChineseEmbedding.txt'], []\n",
      "2019-04-09 02:29:25,438 util_log.py[62] INFO cost 06.5 min:load_embedding(['./Tencent_AILab_ChineseEmbedding.txt'], []), return:('dict:8824330',), end \n",
      "2019-04-09 02:29:25,439 util_log.py[47] INFO filter_duplicate_words begin with(1 paras) :[['./input/data_train.csv']], []\n",
      "2019-04-09 02:29:32,875 <timed exec>[59] DEBUG There are 54039 words after file:./input/data_train.csv\n",
      "2019-04-09 02:29:51,393 <timed exec>[59] DEBUG There are 73572 words after file:./input/data_train.csv\n",
      "2019-04-09 02:29:51,393 <timed exec>[60] DEBUG There are 73572 word were parser from word_count:7023703 file_list:['./input/data_train.csv']\n",
      "2019-04-09 02:29:51,421 util_log.py[62] INFO cost 26.0 sec:filter_duplicate_words([['./input/data_train.csv']], []), return:('list:73572',), end \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 40s, sys: 13.6 s, total: 5min 54s\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embed = load_embedding(word2vec_model)\n",
    "\n",
    "word_list = filter_duplicate_words([train_file,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 02:29:51,428 util_log.py[47] INFO gen_mini_embedding begin with(2 paras) :['dict', 'list'], []\n",
      "2019-04-09 02:29:51,439 <timed exec>[15] DEBUG The word list split to 8 partitions:[9197, 9197, 9197, 9197, 9197, 9197, 9197, 9193]\n",
      "2019-04-09 02:29:51,493 <timed exec>[134] ERROR all ngrams for word \"\" absent from model\n",
      "2019-04-09 02:29:51,616 <timed exec>[78] DEBUG Can not find vec for:0,\n",
      "2019-04-09 02:29:51,848 <timed exec>[130] WARNING Can not find \t in wv\n",
      "2019-04-09 02:29:52,038 <timed exec>[134] ERROR all ngrams for word \"\t\" absent from model\n",
      "2019-04-09 02:29:52,515 <timed exec>[78] DEBUG Can not find vec for:1,\t\n",
      "2019-04-09 02:29:55,205 <timed exec>[130] WARNING Can not find 《 in wv\n",
      "2019-04-09 02:29:55,223 <timed exec>[134] ERROR all ngrams for word \"《\" absent from model\n",
      "2019-04-09 02:29:55,251 <timed exec>[78] DEBUG Can not find vec for:1,《\n",
      "2019-04-09 02:30:02,250 <timed exec>[130] WARNING Can not find ， in wv\n",
      "2019-04-09 02:30:02,252 <timed exec>[134] ERROR all ngrams for word \"，\" absent from model\n",
      "2019-04-09 02:30:02,252 <timed exec>[78] DEBUG Can not find vec for:1,，\n",
      "2019-04-09 02:30:02,458 util_log.py[62] INFO cost 11.0 sec:gen_mini_embedding(['dict', 'list'], []), return:DF:(73572, 200), end \n",
      "2019-04-09 02:30:02,459 <timed exec>[3] DEBUG The length of the vector is (73572, 200)\n",
      "2019-04-09 02:30:09,992 <timed exec>[12] INFO Mini dict save to ./output/mini.kv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 2.78 s, total: 18.1 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = gen_mini_embedding(embed, word_list)\n",
    "logger.debug(f'The length of the vector is {data.shape}')\n",
    "\n",
    "fname = \"./output/mini.kv\"\n",
    "np.savetxt(fname, data.reset_index().values,\n",
    "           delimiter=\" \",\n",
    "           header=\"{} {}\".format(len(data), len(data.columns)),\n",
    "           comments=\"\",\n",
    "           fmt=[\"%s\"] + [\"%.6f\"] * len(data.columns))\n",
    "\n",
    "logger.info(f'Mini dict save to {fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 02:30:11,251 textcleaner.py[37] INFO 'pattern' package not found; tag filters are not available for English\n",
      "2019-04-09 02:30:11,258 utils_any2vec.py[170] INFO loading projection weights from ./output/mini.kv\n",
      "2019-04-09 02:30:11,259 smart_open_lib.py[149] DEBUG {'kw': {}, 'mode': 'rb', 'uri': './output/mini.kv'}\n",
      "2019-04-09 02:30:11,281 smart_open_lib.py[621] DEBUG encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='./output/mini.kv'>}\n",
      "2019-04-09 02:30:21,048 utils_any2vec.py[232] INFO loaded (73572, 200) matrix from ./output/mini.kv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.6 s, sys: 982 ms, total: 10.6 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 02:30:21,052 keyedvectors.py[1360] INFO precomputing L2-norms of word weight vectors\n",
      "/users/hdpsbp/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py:1366: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.vectors_norm = (self.vectors / sqrt((self.vectors ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/users/hdpsbp/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('看望', 0.9192631244659424),\n",
       " ('探视', 0.8276310563087463),\n",
       " ('拜访', 0.7015642523765564),\n",
       " ('慰问', 0.7004474401473999),\n",
       " ('探亲', 0.6589413285255432),\n",
       " ('病重', 0.6578432321548462),\n",
       " ('问安', 0.6455398201942444),\n",
       " ('照料', 0.6373718976974487),\n",
       " ('小住', 0.6348545551300049),\n",
       " ('病危', 0.6343668699264526)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('探望')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/hdpsbp/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('系统对', 0.7760351300239563),\n",
       " ('软件系统', 0.7727014422416687),\n",
       " ('管理系统', 0.7511553764343262),\n",
       " ('对系统', 0.7509126663208008),\n",
       " ('功能', 0.7172309160232544),\n",
       " ('模块', 0.7166910171508789),\n",
       " ('操作系统', 0.7163224816322327),\n",
       " ('信息系统', 0.7055225968360901),\n",
       " ('设备', 0.6992197036743164),\n",
       " ('应用', 0.6786333918571472)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('系统')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21462 ,  0.139884,  0.391754,  0.231987,  0.11961 , -0.312598,\n",
       "       -0.102125,  0.020797,  0.361761, -0.09875 , -0.047344,  0.129789,\n",
       "        0.219998, -0.407498,  0.161724, -0.118959, -0.192629, -0.294972,\n",
       "       -0.136594, -0.199864,  0.082437,  0.378111,  0.022619,  0.093166,\n",
       "        0.104366,  0.571521, -0.243688,  0.446157,  0.359877,  0.052411,\n",
       "       -0.06698 ,  0.102023,  0.178072,  0.049176, -0.301317, -0.044619,\n",
       "        0.053053,  0.23609 , -0.085294, -0.035479,  0.335128, -0.326366,\n",
       "       -0.090003,  0.744669, -0.345959, -0.213992, -0.287085, -0.381243,\n",
       "        0.087105,  0.069898, -0.175255, -0.233278, -0.172423,  0.10459 ,\n",
       "        0.283894,  0.354634,  0.032908,  0.140956, -0.14814 ,  0.204862,\n",
       "       -0.186671,  0.242083,  0.620884,  0.150089,  0.072815, -0.047854,\n",
       "       -0.078099, -0.216561, -0.162142, -0.011308, -0.762996, -0.124233,\n",
       "        0.127123,  0.445678, -0.0127  ,  0.008824,  0.343807, -0.144965,\n",
       "       -0.134015,  0.321934,  0.113197,  0.022872, -0.057701,  0.68145 ,\n",
       "       -0.313112,  0.001493,  0.197529, -0.012771, -0.009035,  0.198813,\n",
       "        0.128235,  0.048779,  0.317918, -0.186882,  0.006798,  0.053179,\n",
       "        0.118678, -0.148397, -0.070585, -0.230141, -0.353029, -0.024326,\n",
       "       -0.163439, -0.067331, -0.300373,  0.209197,  0.039434, -0.360569,\n",
       "        0.428135, -0.070241,  0.101072,  0.145795, -0.363169,  0.188817,\n",
       "        0.154864,  0.06418 , -0.077153, -0.283427, -0.068774, -0.122014,\n",
       "        0.108701,  0.024629, -0.164759, -0.097373,  0.161753,  0.400396,\n",
       "       -0.387667, -0.077576, -0.301773, -0.300575,  0.301848,  0.23831 ,\n",
       "        0.011117,  0.141876, -0.265925, -0.450643, -0.093379, -0.047365,\n",
       "       -0.47403 , -0.037783, -0.024907,  0.080771, -0.75898 , -0.112935,\n",
       "       -0.182023, -0.011752,  0.170578,  0.136287, -0.045986, -0.264574,\n",
       "       -0.28469 , -0.456078, -0.185575,  0.227772, -0.427051,  0.323368,\n",
       "       -0.0069  , -0.003334,  0.582234, -0.184942, -0.114188,  0.214392,\n",
       "        0.525155, -0.170885,  0.320589, -0.021454,  0.079907,  0.56473 ,\n",
       "        0.417533, -0.00219 ,  0.451339,  0.240222, -0.2033  ,  0.12175 ,\n",
       "       -0.053368, -0.431225,  0.216268, -0.555037,  0.264816,  0.041348,\n",
       "        0.272905, -0.393007, -0.501606, -0.090904,  0.013922,  0.080536,\n",
       "       -0.101174,  0.207017,  0.036583, -0.399987,  0.149297,  0.030457,\n",
       "       -0.161488,  0.246505,  0.365593,  0.356233, -0.600165, -0.610948,\n",
       "       -0.675905, -0.070546], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['探望']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/hdpsbp/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('年末', 0.8267966508865356),\n",
       " ('下半年', 0.7590392827987671),\n",
       " ('上半年', 0.7561988830566406),\n",
       " ('月底', 0.7522507905960083),\n",
       " ('年初', 0.7464196681976318),\n",
       " ('年关', 0.7323346734046936),\n",
       " ('元旦', 0.728956401348114),\n",
       " ('今年年底', 0.7181286811828613),\n",
       " ('明年', 0.6944761872291565),\n",
       " ('春节', 0.686357319355011),\n",
       " ('临近', 0.6640993356704712),\n",
       " ('年终', 0.6542741656303406),\n",
       " ('中旬', 0.6485908031463623),\n",
       " ('开年', 0.6462185382843018),\n",
       " ('三月份', 0.6397310495376587),\n",
       " ('元旦节', 0.6288689970970154),\n",
       " ('一月份', 0.6273951530456543),\n",
       " ('月末', 0.626624345779419),\n",
       " ('今年', 0.6242791414260864),\n",
       " ('春节假期', 0.6214699149131775)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('年底',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02014',\n",
       " '03',\n",
       " '0309',\n",
       " '036273',\n",
       " '04',\n",
       " '041928',\n",
       " '0440',\n",
       " '0444',\n",
       " '04816',\n",
       " '05']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_vectors.vocab.keys())[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vectors.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
